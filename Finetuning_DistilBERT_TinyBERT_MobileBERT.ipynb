{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPrH1QsTkQTpAMCM0Km2J1h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peravali810/FinetuningLLMs/blob/main/Finetuning_DistilBERT_TinyBERT_MobileBERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UJf9vyRk3aGJ"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers accelerate datasets bertviz umap-learn seaborn openpyxl evaluate\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Loading dataset"
      ],
      "metadata": {
        "id": "Mpfa-ph15KD4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_excel(\"https://github.com/laxmimerit/All-CSV-ML-Data-Files-Download/raw/master/fake_news.xlsx\")"
      ],
      "metadata": {
        "id": "Fm12aOne5GbJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df = df.dropna()\n",
        "\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "0m9Bz_WB66qq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "QQRH-vkYDCTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['label'].value_counts()"
      ],
      "metadata": {
        "id": "0_7q0mdADGoP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Dataset Analyis"
      ],
      "metadata": {
        "id": "WGXJYlO4DS30"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "labels = df['label'].value_counts(ascending=True)\n",
        "labels.plot.barh()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Xv9NP-8dDWq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['title_tokens'] = df[\"title\"].apply(lambda x: len(x.split())*1.5)\n",
        "df['text_tokens'] = df['text'].apply(lambda x: len(x.split())*1.5)\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots(1,2, figsize=(15,5))\n",
        "ax[0].hist(df['title_tokens'], bins = 50, color= 'skyblue')\n",
        "ax[0].set_title('Title Tokens')\n",
        "\n",
        "ax[1].hist(df['text_tokens'], bins = 50, color= 'red')\n",
        "ax[1].set_title('Text Tokens')"
      ],
      "metadata": {
        "id": "hsM76ZxCFU2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "bOVIxsaNGQG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Creating DataLoader and Train Test Split"
      ],
      "metadata": {
        "id": "EuBNTLmuJVN_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train, test = train_test_split(df, test_size = 0.3, stratify = df['label'])\n",
        "test, validation = train_test_split(test, test_size=1/3, stratify = test['label'])\n",
        "\n",
        "train.shape, test.shape, validation.shape, df.shape"
      ],
      "metadata": {
        "id": "4mxEFzGbJd3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset, DatasetDict\n",
        "\n",
        "dataset = DatasetDict(\n",
        "    {\n",
        "        'train' : Dataset.from_pandas(train, preserve_index=False),\n",
        "        'test' : Dataset.from_pandas(test, preserve_index=False),\n",
        "        'validation' : Dataset.from_pandas(validation, preserve_index=False)\n",
        "    }\n",
        "\n",
        ")\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "id": "FHo8bZbMK9X5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Creating data tokens(Tokenization)"
      ],
      "metadata": {
        "id": "FHNWVtF1MVlA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer"
      ],
      "metadata": {
        "id": "qppkJEF0NSc2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Hi, how are you ?\"\n",
        "\n",
        "model_ckpt = 'distilbert-base-uncased'\n",
        "dtokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
        "dtokens = dtokenizer.tokenize(text)\n",
        "\n",
        "model_ckpt = 'google/mobilebert-uncased'\n",
        "mtokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
        "mtokens = mtokenizer.tokenize(text)\n",
        "\n",
        "model_ckpt = 'Intel/dynamic_tinybert'\n",
        "ttokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
        "ttokens = ttokenizer.tokenize(text)"
      ],
      "metadata": {
        "id": "GhMOE8X_MVHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(batch):\n",
        "\n",
        "    temp = dtokenizer(batch['title'], padding = True, truncation = True)\n",
        "    return temp\n",
        "\n",
        "print(tokenize(dataset['train'][:2]))"
      ],
      "metadata": {
        "id": "k3Cka2TQNWZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "enDataset = dataset.map(tokenize, batched=True, batch_size=None)"
      ],
      "metadata": {
        "id": "eZl9VsQDQ6Pj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Building model"
      ],
      "metadata": {
        "id": "b3_r7yZAR6tR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoConfig\n",
        "import torch\n",
        "\n",
        "id2label = {0:'Real', 1:'Fake'}\n",
        "label2id = {'Real':0, 'Fake':1}\n",
        "\n",
        "model_ckpt = 'distilbert-base-uncased'\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "config = AutoConfig.from_pretrained(model_ckpt, id2label=id2label, label2id=label2id)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_ckpt, config=config).to(device)\n",
        "\n"
      ],
      "metadata": {
        "id": "04hJXxDQR270"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.config"
      ],
      "metadata": {
        "id": "w4LMdbIoTufC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "batch_size = 64\n",
        "training_dir = \"bert_base_train_dir\"\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=training_dir,\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=2,\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    weight_decay=0.01,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    disable_tqdm=False,\n",
        "    report_to=\"none\"\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "RCS-rDTGTtS1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "import numpy as np\n",
        "\n",
        "accuracy = evaluate.load(\"accuracy\")\n",
        "\n",
        "def eval_compute_metrics(eval_pred):\n",
        "  predictions, labels = eval_pred\n",
        "  predictions = np.argmax(predictions, axis=1)\n",
        "\n",
        "  return accuracy.compute(predictions=predictions, references=labels)"
      ],
      "metadata": {
        "id": "aRBUSu6yUJVJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model = model,\n",
        "    args = training_args,\n",
        "    compute_metrics = eval_compute_metrics,\n",
        "    train_dataset = enDataset['train'],\n",
        "    eval_dataset = enDataset['validation'],\n",
        "    tokenizer = dtokenizer\n",
        ")"
      ],
      "metadata": {
        "id": "3nuNgdwDUfq-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "EDUg5GSeUxHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model Evaluation"
      ],
      "metadata": {
        "id": "kTJ-Mo5aU8b7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "OutPreds =  trainer.predict(enDataset['test'])\n",
        "OutPreds.metrics"
      ],
      "metadata": {
        "id": "3K74hgM-U-PP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = np.argmax(OutPreds.predictions, axis=1)\n",
        "y_true = enDataset['test'][:]['label']"
      ],
      "metadata": {
        "id": "8dkw61p6VPSa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(y_true, y_pred, target_names=list(label2id)))"
      ],
      "metadata": {
        "id": "K40qYqOmVVZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Benchmarking all models"
      ],
      "metadata": {
        "id": "M52o_tmeV7zl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "modelDict = {\n",
        "    'distilbert' : 'distilbert-base-uncased',\n",
        "    'mobilebert' : 'google/mobilebert-uncased',\n",
        "    'tinybert' : 'Intel/dynamic_tinybert'\n",
        "}\n",
        "\n",
        "def train(model):\n",
        "  model_ckpt = modelDict[model]\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
        "  localmodel = AutoModelForSequenceClassification.from_pretrained(model_ckpt, config=config).to(device)\n",
        "\n",
        "  def localtokenize(batch):\n",
        "\n",
        "    temp = tokenizer(batch['title'], padding = True, truncation = True)\n",
        "    return temp\n",
        "\n",
        "  enDataset = dataset.map(localtokenize, batched=True, batch_size=None)\n",
        "\n",
        "\n",
        "  trainer = Trainer(\n",
        "    model = localmodel,\n",
        "    args = training_args,\n",
        "    compute_metrics = eval_compute_metrics,\n",
        "    train_dataset = enDataset['train'],\n",
        "    eval_dataset = enDataset['validation'],\n",
        "    tokenizer = tokenizer\n",
        "  )\n",
        "\n",
        "  trainer.train()\n",
        "  OutPreds =  trainer.predict(enDataset['test'])\n",
        "  print(f\"Training completed: {model}\")\n",
        "  return OutPreds.metrics\n",
        "\n",
        "\n",
        "performance = {}\n",
        "for model in modelDict:\n",
        "  print(f\"Training model: {model}\")\n",
        "  temp = train(model)\n",
        "  performance[model] = temp"
      ],
      "metadata": {
        "id": "icWxQwB5WDgK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "performance"
      ],
      "metadata": {
        "id": "EJ9XGFtLbcBR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}